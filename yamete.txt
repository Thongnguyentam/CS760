Question:
Consider the following 3-layer neural network.

\[
\hat{y} = f(x) = g(W_3\sigma(W_2\sigma(W_1x)))
\]

Suppose $x \in \mathbb{R}^d$, $W_1 \in \mathbb{R}^{d_1 \times d}$, $W_2 \in \mathbb{R}^{d_2 \times d_1}$, $W_3 \in \mathbb{R}^{k \times d_2}$ i.e. $f: \mathbb{R}^d \rightarrow \mathbb{R}^k$, Let $\sigma(z) = [\sigma(z_1), ..., \sigma(z_n)]$ for any $z \in \mathbb{R}^n$ where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid (logistic) activation function and $g(z_i) = \frac{exp(z_i)}{\sum_{i=1}^k exp(z_i)}$ is the softmax function. Suppose the true pair is $(x, y)$ where $y \in \{0, 1\}^k$ with exactly one of the entries equal to 1, and you are working with the cross-entropy loss function given below,

\[
L(x, y) = -\sum_{i=1}^k y \text{log}(\hat{y})
\]

\begin{enumerate}
    \item Derive backpropagation updates for the above neural network. (5 pts)

> Is my first part of answer having any mistakes? How to compute the next layer derivation
 To perform backpropagation, we need to compute the gradients of the loss function with respect to the parameters of the network. Starting by computing the gradients of the loss with respect to the output $\hat{y}$:

        \begin{align*}
        \frac{\partial L}{\partial \hat{y}_i} 
        &= -\frac{y_j}{\hat{y}_j}.
        \end{align*}
        
        Using the chain rule, we can now compute the gradients of the loss with respect to the weights $W_3$:
        
        \begin{align*}
        \frac{\partial L}{\partial W_{3_{mn}}} &= \sum_{l=1}^k \frac{\partial L}{\partial \hat{y}_{l}} \frac{\partial \hat{y}_{l}}{\partial  W_{3_{mn}}} \\
        &= \sum_{l=1}^k \left(-\frac{y_{l}}{\hat{y}_{l}}\right) \frac{\partial \hat{y}_{l}}{\partial  W_{3_{mn}}} \\
        &= \left(-\frac{y_{j}}{\hat{y}_{j}}\right) \frac{\partial y_{j}}{\partial  W_{3_{mn}}} \\
        & = \left(-\frac{y_{j}}{\hat{y}_{j}}\right) \hat{y}_{n} (\delta_{mn}-\hat{y}_{j}) \frac{W_3_{mn}\sigma(W_2\sigma(W_1x))}{\partial  W_{3_{mn}}} \\
        & = \left(-\frac{y_{j}}{\hat{y}_{j}}\right) \hat{y}_{n} (\delta_{mn}-\hat{y}_{j}) \sigma(W_2\sigma(W_1x))
    \end{align*}
        where j is the index of the correct class, $\delta_{mn}$ is the Kronecker delta, which equals 1 if $n=j$ and 0 otherwise. \\