{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 0.3333333333333333, 's': 0.3333333333333333, 'j': 0.3333333333333333}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'languageID'\n",
    "alpha = 1/2 \n",
    "K_l = 3\n",
    "num_sample = {'e': 10, 's': 10, 'j' : 10}\n",
    "prior = {}\n",
    "for label in num_sample:\n",
    "    cnt = num_sample[label]\n",
    "    prior[label] = (cnt + alpha)/ (sum(num_sample.values()) + (K_l*alpha))\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = []\n",
    "dict_list = []\n",
    "for label in ['e','s','j']:\n",
    "    alpha_cnt = {}\n",
    "    for i in range (0, 10):\n",
    "        name = label + (str(i)) + \".txt\"\n",
    "        file_path = os.path.join('languageID', name)\n",
    "        with open(file_path,'r') as f:\n",
    "            content = f.read()\n",
    "            for char in content:\n",
    "                if char.isalpha() or char.isspace():\n",
    "                    if char in alpha_cnt:\n",
    "                        alpha_cnt[char] += 1\n",
    "                    else:\n",
    "                        alpha_cnt[char] = 1\n",
    "    #sort keys\n",
    "    if '\\n' in alpha_cnt:\n",
    "        del alpha_cnt['\\n']\n",
    "\n",
    "    sorted_dict = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "    #finding theta\n",
    "    deno = sum(sorted_dict.values()) + alpha*len(sorted_dict)\n",
    "    theta = []\n",
    "    for val in sorted_dict:\n",
    "        i = (sorted_dict[val] + alpha)/ deno\n",
    "        theta.append(i)\n",
    "    dict_list.append(sorted_dict)\n",
    "    lang.append(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1766,\n",
       " 'a': 1885,\n",
       " 'b': 155,\n",
       " 'c': 78,\n",
       " 'd': 246,\n",
       " 'e': 861,\n",
       " 'f': 55,\n",
       " 'g': 200,\n",
       " 'h': 454,\n",
       " 'i': 1388,\n",
       " 'j': 33,\n",
       " 'k': 821,\n",
       " 'l': 20,\n",
       " 'm': 569,\n",
       " 'n': 811,\n",
       " 'o': 1304,\n",
       " 'p': 12,\n",
       " 'q': 1,\n",
       " 'r': 612,\n",
       " 's': 603,\n",
       " 't': 815,\n",
       " 'u': 1010,\n",
       " 'v': 3,\n",
       " 'w': 282,\n",
       " 'y': 202,\n",
       " 'z': 110}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1792499586981662,\n",
       " 0.0601685114819098,\n",
       " 0.011134974392863043,\n",
       " 0.021509995043779945,\n",
       " 0.021972575582355856,\n",
       " 0.1053692383941847,\n",
       " 0.018932760614571286,\n",
       " 0.017478936064761277,\n",
       " 0.047216256401784236,\n",
       " 0.055410540227986124,\n",
       " 0.001420783082768875,\n",
       " 0.0037336857756484387,\n",
       " 0.028977366595076822,\n",
       " 0.020518751032545846,\n",
       " 0.057921691723112505,\n",
       " 0.06446390219725756,\n",
       " 0.01675202378985627,\n",
       " 0.0005617049396993227,\n",
       " 0.053824549810011564,\n",
       " 0.06618205848339666,\n",
       " 0.08012555757475633,\n",
       " 0.026664463902197257,\n",
       " 0.009284652238559392,\n",
       " 0.015496448042293078,\n",
       " 0.001156451346439782,\n",
       " 0.013844374690236246,\n",
       " 0.0006277878737815959]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16826493170115014,\n",
       " 0.10456045141993771,\n",
       " 0.008232863618143134,\n",
       " 0.03752582405722919,\n",
       " 0.039745922111559924,\n",
       " 0.1138108599796491,\n",
       " 0.00860287996053159,\n",
       " 0.0071844839813758445,\n",
       " 0.0045327001942585795,\n",
       " 0.049859702136844375,\n",
       " 0.006629459467793161,\n",
       " 0.0002775122567913416,\n",
       " 0.052943171656748174,\n",
       " 0.02580863988159477,\n",
       " 0.054176559464709693,\n",
       " 0.07249236841293824,\n",
       " 0.02426690512164287,\n",
       " 0.007677839104560451,\n",
       " 0.05929511886774999,\n",
       " 0.06577040485954797,\n",
       " 0.03561407295488884,\n",
       " 0.03370232185254849,\n",
       " 0.00588942678301625,\n",
       " 9.250408559711388e-05,\n",
       " 0.0024976103111220747,\n",
       " 0.007862847275754679,\n",
       " 0.0026826184823163022]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s\n",
    "lang[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12345377035432245,\n",
       " 0.13177021455028304,\n",
       " 0.010867286323293032,\n",
       " 0.005486057725906772,\n",
       " 0.01722692012020407,\n",
       " 0.06020686281361381,\n",
       " 0.003878677755258928,\n",
       " 0.014012160178908379,\n",
       " 0.031763225941715004,\n",
       " 0.09703682996715354,\n",
       " 0.002341183870291425,\n",
       " 0.05741141938640017,\n",
       " 0.0014326647564469914,\n",
       " 0.039800125794954226,\n",
       " 0.05671255852959676,\n",
       " 0.09116639877000489,\n",
       " 0.0008735760710042631,\n",
       " 0.00010482912852051157,\n",
       " 0.04280522747920889,\n",
       " 0.042176252708085823,\n",
       " 0.05699210287231812,\n",
       " 0.07061988957998462,\n",
       " 0.00024460129988119366,\n",
       " 0.019742819204696345,\n",
       " 0.014151932350269061,\n",
       " 0.007722412467677685]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#j\n",
    "lang[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4\n",
    "alpha_cnt = {}\n",
    "name = 'e10.txt'\n",
    "file_path = os.path.join('languageID', name)\n",
    "with open(file_path,'r') as f:\n",
    "    content = f.read()\n",
    "    for char in content:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            if char in alpha_cnt:\n",
    "                alpha_cnt[char] += 1\n",
    "            else:\n",
    "                alpha_cnt[char] = 1\n",
    "#sort keys\n",
    "if '\\n' in alpha_cnt:\n",
    "    del alpha_cnt['\\n']\n",
    "\n",
    "x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "#finding theta\n",
    "deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "theta_test = []\n",
    "for val in x_test:\n",
    "    i = (x_test[val] + alpha)/ deno\n",
    "    theta_test.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 498,\n",
       " 'a': 164,\n",
       " 'b': 32,\n",
       " 'c': 53,\n",
       " 'd': 57,\n",
       " 'e': 311,\n",
       " 'f': 55,\n",
       " 'g': 51,\n",
       " 'h': 140,\n",
       " 'i': 140,\n",
       " 'j': 3,\n",
       " 'k': 6,\n",
       " 'l': 85,\n",
       " 'm': 64,\n",
       " 'n': 139,\n",
       " 'o': 182,\n",
       " 'p': 53,\n",
       " 'q': 3,\n",
       " 'r': 141,\n",
       " 's': 186,\n",
       " 't': 225,\n",
       " 'u': 65,\n",
       " 'v': 31,\n",
       " 'w': 47,\n",
       " 'x': 4,\n",
       " 'y': 38,\n",
       " 'z': 2}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7841.865447060634, -8467.282044010557, -8730.288949147405]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log likelihood\n",
    "p_list = []\n",
    "language = 0\n",
    "for label in ['e', 's', 'j']:\n",
    "    i = 0\n",
    "    p = 0\n",
    "    for x in x_test:\n",
    "        if x == list(dict_list[language].keys())[i]:\n",
    "            p = p + x_test[x]*np.log(lang[language][i])\n",
    "            i = i + 1\n",
    "    p_list.append(p) \n",
    "    language = language + 1\n",
    "p_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['e', 's', 'j'] \n",
    "#log likelihood\n",
    "p_list = []\n",
    "language = 0\n",
    "for label in ['e', 's', 'j']:\n",
    "    i = 0\n",
    "    p = 0\n",
    "    for x in x_test:\n",
    "        if x == list(dict_list[language].keys())[i]:\n",
    "            p = p + x_test[x]*np.log(lang[language][i])\n",
    "            i = i + 1\n",
    "    p_list.append(p) \n",
    "    language = language + 1\n",
    "p_list\n",
    "\n",
    "#log posterior\n",
    "p_list_post = p_list + np.log(list(prior.values()))\n",
    " \n",
    "#show the class label (Choose the class with the highest posterior probability as the predicted class label for x)\n",
    "ind_max = np.argmax(p_list_post)\n",
    "label_list[ind_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#test files 10.txt to 19.txt in three languages\n",
    "ee, ss, jj, ej, es, js, je, se, sj = 0,0,0,0,0,0,0,0,0\n",
    "for label in ['e', 's', 'j']:\n",
    "    for i in range(10, 20):\n",
    "        alpha_cnt = {}\n",
    "        name = label + str(i) + '.txt'\n",
    "        file_path = os.path.join('languageID', name)\n",
    "        with open(file_path,'r') as f:\n",
    "            content = f.read()\n",
    "            for char in content:\n",
    "                if char.isalpha() or char.isspace():\n",
    "                    if char in alpha_cnt:\n",
    "                        alpha_cnt[char] += 1\n",
    "                    else:\n",
    "                        alpha_cnt[char] = 1\n",
    "        #sort keys\n",
    "        if '\\n' in alpha_cnt:\n",
    "            del alpha_cnt['\\n']\n",
    "\n",
    "        x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "        #finding theta\n",
    "        deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "        theta_test = []\n",
    "        for val in x_test:\n",
    "            i = (x_test[val] + alpha)/ deno\n",
    "            theta_test.append(i)\n",
    "        \n",
    "        #log likelihood\n",
    "        p_list = []\n",
    "        language = 0\n",
    "        for label2 in ['e', 's', 'j']:\n",
    "            i = 0\n",
    "            p = 0\n",
    "            for x in x_test:\n",
    "                if x == list(dict_list[language].keys())[i]:\n",
    "                    p = p + x_test[x]*np.log(lang[language][i])\n",
    "                    i = i + 1\n",
    "            p_list.append(p) \n",
    "            language = language + 1\n",
    "        \n",
    "        #log posterior\n",
    "        p_list_post = p_list + np.log(list(prior.values()))\n",
    "\n",
    "        #show the class label (Choose the class with the highest posterior probability as the predicted class label for x)\n",
    "        ind_max = np.argmax(p_list_post)\n",
    "        clas = label_list[ind_max]\n",
    "        #evaluate\n",
    "        if label == 'e' and clas == 'e':\n",
    "            ee += 1\n",
    "        if label == 'e' and clas == 'j':\n",
    "            ej += 1\n",
    "        if label == 'e' and clas == 's':\n",
    "            es += 1\n",
    "        if label == 'j' and clas == 'j':\n",
    "            jj += 1\n",
    "        if label == 'j' and clas == 's':\n",
    "            js += 1\n",
    "        if label == 'j' and clas == 'e':\n",
    "            je += 1\n",
    "        if label == 's' and clas == 's':\n",
    "            ss += 1\n",
    "        if label == 's' and clas == 'j':\n",
    "            sj += 1\n",
    "        if label == 's' and clas == 'e':\n",
    "            se += 1\n",
    "print(ee, ss, jj, ej, es, js, je, se, sj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 498,\n",
       " 'a': 164,\n",
       " 'b': 32,\n",
       " 'c': 53,\n",
       " 'd': 57,\n",
       " 'e': 311,\n",
       " 'f': 55,\n",
       " 'g': 51,\n",
       " 'h': 140,\n",
       " 'i': 140,\n",
       " 'j': 3,\n",
       " 'k': 6,\n",
       " 'l': 85,\n",
       " 'm': 64,\n",
       " 'n': 139,\n",
       " 'o': 182,\n",
       " 'p': 53,\n",
       " 'q': 3,\n",
       " 'r': 141,\n",
       " 's': 186,\n",
       " 't': 225,\n",
       " 'u': 65,\n",
       " 'v': 31,\n",
       " 'w': 47,\n",
       " 'x': 4,\n",
       " 'y': 38,\n",
       " 'z': 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.8\n",
    "alpha_cnt = {}\n",
    "name = 'e10.txt'\n",
    "file_path = os.path.join('languageID', name)\n",
    "with open(file_path,'r') as f:\n",
    "    content = f.read()\n",
    "    shuffled_content = ''.join(random.sample(content, len(content)))\n",
    "    for char in content:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            if char in alpha_cnt:\n",
    "                alpha_cnt[char] += 1\n",
    "            else:\n",
    "                alpha_cnt[char] = 1\n",
    "#sort keys\n",
    "if '\\n' in alpha_cnt:\n",
    "    del alpha_cnt['\\n']\n",
    "\n",
    "x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "#finding theta\n",
    "deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "theta_test = []\n",
    "for val in x_test:\n",
    "    i = (x_test[val] + alpha)/ deno\n",
    "    theta_test.append(i)\n",
    "x_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "#criterion is a PyTorch loss function that computes the loss between the predicted output \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparemeters\n",
    "batch_size = 32\n",
    "batch_size_test = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "input_size = 784  #28x28 pixels\n",
    "hidden_size1 = 300\n",
    "hidden_size2 = 200\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST dataset \n",
    "#TorchVision also offers a lot of handy transformations, such as cropping or normalization.\n",
    "train_dataset = datasets.MNIST(root= './data', train= True, transform= transforms.ToTensor(), download= True)\n",
    "test_dataset = datasets.MNIST(root = './data', train = False, transform= transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data loaders\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size= batch_size, shuffle= True)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "#The torch.randn function generates a tensor of size (hidden_size1, input_size) \n",
    "#filled with random values drawn from a normal distribution with mean 0 and standard deviation 1\n",
    "#The division by np.sqrt(input_size) scales the initial weights by the square root of the input size, \n",
    "#which is a common practice to ensure that the initial weights are not too large or too small.\n",
    "W1 = torch.randn(hidden_size1, input_size) / np.sqrt(input_size)\n",
    "W2 = torch.randn(hidden_size2, hidden_size1) / np.sqrt(hidden_size1)\n",
    "W3 = torch.randn(num_classes, hidden_size2) / np.sqrt(hidden_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    z1 = torch.matmul(W1, x)\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.matmul(W2, a1)\n",
    "    a2 = torch.sigmoid(z2)\n",
    "    z3 = torch.matmul(W3, a2)\n",
    "    y_hat = torch.softmax(z3, dim = 0)\n",
    "    return a1, a2, y_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((num_classes, batch_size))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thong\\AppData\\Local\\Temp\\ipykernel_19000\\3774397082.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(images), torch.tensor(y) #convert 'images' and 'y' into PyTorch tensors.\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # batch_index, (example_data, example_targets)\n",
    "    # labels is a tensor of shape (batch_size,) that contains the true labels for a batch of images.\n",
    "    # images.shape = torch.Size([128, 1, 28, 28])\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #flatten the images:\n",
    "        images = images.reshape(-1, input_size).T #turn into a vector\n",
    "        labels = labels.numpy() # converting the labels tensor from a PyTorch tensor object to a numpy array.\n",
    "\n",
    "        #convert labels to one-hot vectors\n",
    "        y = np.zeros((num_classes, batch_size))\n",
    "        batch_size_i = labels.shape[0]\n",
    "        y[labels, np.arange(batch_size_i)] = 1 #last batch of batch_size = 128 is 96\n",
    "        if batch_size_i < batch_size:\n",
    "            y = np.delete(y, np.arange(batch_size_i, batch_size), axis = 1) #delete the remaining unsed columns\n",
    "        x, y = torch.tensor(images), torch.tensor(y) #convert 'images' and 'y' into PyTorch tensors.\n",
    "\n",
    "        #forward pass \n",
    "        a1, a2, y_hat = forward(x)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = -torch.sum(y * torch.log(y_hat))\n",
    "\n",
    "        #backward pass\n",
    "        delta3 = (y_hat - y).float() #change from double to float \n",
    "        delta2 = torch.matmul(W3.T, delta3)* a2 * (1- a2)\n",
    "        delta1 = torch.matmul(W2.T, delta2) * a1 * (1- a1)\n",
    "\n",
    "        dW3 = torch.matmul(delta3, a2.T) #(y_hat - y)a2^T\n",
    "        dW2 = torch.matmul(delta2, a1.T) # W3.T(y_hat - y)a2(1- a2)a1.T\n",
    "        dw1 = torch.matmul(delta1, x.T) # W2.T*delta2*a1*(1- a1)x.T\n",
    "\n",
    "        #update weights using SGD  \n",
    "        W1 -= learning_rate* dw1 / batch_size\n",
    "        W2 -= learning_rate* dW2 / batch_size\n",
    "        W3 -= learning_rate * dW3 / batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 0, 3, 7, 4, 2, 6, 7, 5, 8, 7, 9, 6, 0, 1, 7, 7, 7, 5, 8, 1,\n",
       "       6, 1, 1, 1, 2, 8, 7, 8, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "np.arange(batch_size_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "test0 = np.zeros((num_classes, batch_size))\n",
    "test0 #10 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "test0[labels, np.arange(batch_size_i)] = 1 #set test0[label[i], i] = 1\n",
    "test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thong\\AppData\\Local\\Temp\\ipykernel_19000\\781778586.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(images), torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0 \n",
    "test_losses = []\n",
    "\n",
    "# is a context manager that temporarily disables gradient computation during model inference. \n",
    "# reduce memory consumption and computation time since gradients are not needed for inference.\n",
    "# is used to compute test accuracy and loss during the testing phase.\n",
    "# when you are sure that you will not call Tensor.backward() \n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        #Flatten the images\n",
    "        images = images.reshape(-1, input_size).T\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        #convert labels into one-hot vectors\n",
    "        y = np.zeros((num_classes, batch_size))\n",
    "        batch_size_j = labels.shape[0]\n",
    "        y[labels,np.arange(batch_size_j)] = 1\n",
    "        if batch_size_j < batch_size:\n",
    "            y = np.delete(y, np.arange(batch_size_j, batch_size), axis = 1)\n",
    "        \n",
    "        x, y = torch.tensor(images), torch.tensor(y)\n",
    "\n",
    "        #forward pass\n",
    "        _, _, y_hat = forward(x)\n",
    "\n",
    "        # compute loss\n",
    "        #This function returns the index of the maximum value in each column of the one-hot encoded y tensor.\n",
    "        #The dim=0 argument specifies that the function should return the index along the first dimension of the tensor, which corresponds to the batch size.\n",
    "        test_loss = criterion(y_hat, torch.argmax(y, dim = 0))\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        #compute predictions and accuracy \n",
    "        # predictions is a tensor of shape (1, batch_size). \n",
    "        # Each element in this tensor represents the predicted class label for the corresponding input image in the batch\n",
    "        #torch.max() is used to get the index of the maximum value in the tensor y_hat along the 0-th dimension\n",
    "        \n",
    "        _, predictions = torch.max(y_hat, 0)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        total += y.size(1)\n",
    "        correct += (predictions == labels).sum().item()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first dimension = 10 = batch_test_size ->dim = 0 \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim 1 x 10, (batch_size,)\n",
    "torch.argmax(y, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7950e-06, 1.0077e-03, 1.6188e-04, 8.2712e-01, 3.1803e-07, 1.4093e-05,\n",
       "         2.7877e-04, 2.2274e-06, 3.6304e-03, 1.2768e-03],\n",
       "        [1.0209e-04, 2.8491e-05, 1.1066e-06, 1.3787e-07, 9.7752e-01, 8.7183e-05,\n",
       "         4.6971e-05, 4.5300e-05, 4.1221e-03, 2.8811e-07],\n",
       "        [3.6985e-04, 5.2429e-03, 8.3877e-05, 1.3700e-04, 1.1577e-02, 9.9244e-01,\n",
       "         2.8962e-03, 8.1360e-05, 2.2597e-03, 5.4735e-03],\n",
       "        [2.3064e-03, 1.5788e-01, 4.5831e-05, 3.2359e-04, 1.9485e-03, 5.6665e-03,\n",
       "         9.9569e-01, 1.2401e-04, 6.1460e-03, 7.8874e-07],\n",
       "        [1.2725e-05, 5.1942e-06, 5.0506e-01, 3.4314e-06, 2.3034e-05, 1.9848e-07,\n",
       "         1.4108e-08, 7.4706e-01, 7.9489e-03, 3.8632e-04],\n",
       "        [4.2034e-05, 2.1159e-02, 2.0766e-03, 1.6933e-01, 4.0312e-04, 1.2286e-05,\n",
       "         5.6625e-04, 2.7190e-03, 7.5546e-01, 2.5710e-04],\n",
       "        [2.8408e-08, 2.0737e-06, 4.2517e-04, 2.5782e-04, 4.3001e-04, 2.2009e-05,\n",
       "         1.1695e-07, 4.5659e-04, 2.0575e-02, 9.9252e-01],\n",
       "        [9.6345e-01, 5.9457e-05, 5.6157e-03, 1.1751e-06, 2.2564e-04, 5.5439e-06,\n",
       "         3.5710e-04, 9.3528e-04, 3.0361e-04, 2.8109e-08],\n",
       "        [3.7356e-04, 8.1326e-01, 1.9793e-03, 2.8171e-03, 7.7392e-03, 1.7466e-03,\n",
       "         1.5475e-04, 1.8556e-02, 1.9634e-01, 7.7880e-05],\n",
       "        [3.3343e-02, 1.3597e-03, 4.8455e-01, 1.1195e-05, 1.3509e-04, 3.6548e-06,\n",
       "         1.3462e-05, 2.3002e-01, 3.2177e-03, 2.2678e-06]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim 10 x 10, (num_classes, batch_size)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4444947242736816"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y_hat, torch.argmax(y, dim = 0)).item() #item() for getting the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 2.274928773999214\n"
     ]
    }
   ],
   "source": [
    "# Print test accuracy and loss\n",
    "test_accuracy = 100 * correct / total\n",
    "test_loss = np.mean(test_losses)\n",
    "print(test_accuracy, test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network (but with auto-grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model, loss function, and optimizer\n",
    "model = Net(input_size= 784, hidden_size1= 300, hidden_size2 = 200, num_classes = 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Accuracy: 11.35%\n",
      "Epoch 2/20, Accuracy: 20.98%\n",
      "Epoch 3/20, Accuracy: 34.46%\n",
      "Epoch 4/20, Accuracy: 41.57%\n",
      "Epoch 5/20, Accuracy: 52.15%\n",
      "Epoch 6/20, Accuracy: 60.50%\n",
      "Epoch 7/20, Accuracy: 63.81%\n",
      "Epoch 8/20, Accuracy: 65.27%\n",
      "Epoch 9/20, Accuracy: 65.95%\n",
      "Epoch 10/20, Accuracy: 66.24%\n",
      "Epoch 11/20, Accuracy: 66.58%\n",
      "Epoch 12/20, Accuracy: 71.79%\n",
      "Epoch 13/20, Accuracy: 73.02%\n",
      "Epoch 14/20, Accuracy: 73.70%\n",
      "Epoch 15/20, Accuracy: 74.34%\n",
      "Epoch 16/20, Accuracy: 74.84%\n",
      "Epoch 17/20, Accuracy: 75.09%\n",
      "Epoch 18/20, Accuracy: 75.24%\n",
      "Epoch 19/20, Accuracy: 75.52%\n",
      "Epoch 20/20, Accuracy: 75.57%\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "accuracy_list = []\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    ##new## \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ####\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #flatten the images\n",
    "        images = images.reshape(-1, input_size)\n",
    "\n",
    "        #forward pass \n",
    "        outputs = model(images)\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #evaluate the model\n",
    "    with torch.no_grad():\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, input_size)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy:.2f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.56"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num epoch = 25, lr = 0.15, batch_size = 64, accuracy:76.03\n",
    "# num epoch = 25, lr = 0.1, batch_size = 64, accuracy:74.89\n",
    "# num epoch = 20, lr = 0.08, batch_size = 64, accuracy: 66.54\n",
    "# num epoch = 20, lr = 0.15, batch_size = 32, accuracy: \n",
    "# num epoch = 20, lr = 0.1, batch_size = 32, accuracy: \n",
    "# num epoch = 20, lr = 0.07, batch_size = 32, accuracy: 75.57\n",
    "# num epoch = 20, lr = 0.06, batch_size = 32, accuracy:\n",
    "# num epoch = 25, lr = 0.15, batch_size = 32, accuracy: \n",
    "# num epoch = 25, lr = 0.1, batch_size = 32, accuracy: 76.56\n",
    "# num epoch = 25, lr = 0.08, batch_size = 32, accuracy:\n",
    "# num epoch = 25, lr = 0.05, batch_size = 32, accuracy:\n",
    "# num epoch = 30, lr = 0.1, batch_size = 32, accuracy: \n",
    "# num epoch = 30, lr = 0.08, batch_size = 32, accuracy:\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
