{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np \n",
    "from logistic_regression import CustomeLogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#neural network\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVM_Regularize:\n",
    "    def __init__(self, C= 1, learning_rate = 0.001, epochs = 200, batch_size = 64, lmda = 0.1):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lmda = lmda\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X, dtype= torch.float32)\n",
    "        y = torch.tensor(y, dtype = torch.float32)\n",
    "\n",
    "        num_feature = X.shape[1]\n",
    "        self.W = torch.randn(num_feature, 1, requires_grad= True, dtype = torch.float32)\n",
    "        self.b = torch.randn(1, requires_grad= True, dtype = torch.float32)\n",
    "\n",
    "        optimizer = torch.optim.SGD([self.W, self.b], lr = self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            dataloader = DataLoader(TensorDataset(X,y), batch_size= self.batch_size, shuffle= True)\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                # torch.clamp() choose each element to be larger than 0, otherwise 0 \n",
    "                hinge_loss = torch.mean(torch.clamp(1- batch_y * (batch_X @ self.W + self.b), min=0))\n",
    "                reg_loss = 0.5 * self.lmda * torch.sum(self.W ** 2)/self.C #regularization loss\n",
    "                loss = hinge_loss + reg_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype = torch.float32)\n",
    "        return ((torch.sign(X @ self.W  +self.b)+1)/2).detach().numpy()\n",
    "    \n",
    "    def score(self, X , y):\n",
    "        #torch.tensor(y, dtype= torch.float32) makes y into a tensor with size y.shape which is (750, )\n",
    "        y = torch.tensor(y, dtype= torch.float32).unsqueeze(1)\n",
    "        y_pred = torch.tensor(self.predict(X), dtype= torch.float32)\n",
    "        return torch.mean((y_pred == y).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVM_L1(Linear_SVM_Regularize):\n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X, dtype= torch.float32)\n",
    "        y = torch.tensor(y, dtype = torch.float32)\n",
    "\n",
    "        num_feature = X.shape[1]\n",
    "        self.W = torch.randn(num_feature, 1, requires_grad= True, dtype = torch.float32)\n",
    "        self.b = torch.randn(1, requires_grad= True, dtype = torch.float32)\n",
    "\n",
    "        optimizer = torch.optim.SGD([self.W, self.b], lr = self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            dataloader = DataLoader(TensorDataset(X,y), batch_size= self.batch_size, shuffle= True)\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                # torch.clamp() choose each element to be larger than 0, otherwise 0 \n",
    "                hinge_loss = torch.mean(torch.clamp(1- batch_y * (batch_X @ self.W + self.b), min=0))\n",
    "                reg_loss = 0.5 * self.lmda * torch.sum(torch.abs(self.W)) #regularization loss\n",
    "                loss = hinge_loss + reg_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVM(Linear_SVM_Regularize):\n",
    "    def __init__(self, kernel = 'linear', degree = 3, gamma = None, coef0 = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "    \n",
    "    def compute_kernel(self, X1, X2):\n",
    "        if self.kernel == 'linear':\n",
    "            return X1 @ X2.T\n",
    "        \n",
    "        elif self.kernel == 'rbf':\n",
    "            gamma = self.gamma or 1/X1.shape[1]\n",
    "            pairwise_sq_dists = torch.sum(X1 **2, dim=1, keepdim= True) - 2*X1@X2.T + torch.sum(X2 ** 2, dim = 1)\n",
    "            return torch.exp(-gamma * pairwise_sq_dists)\n",
    "        \n",
    "        elif self.kernel == 'polynomial':\n",
    "            return (X1@X2.T * self.gamma + self.coef0) ** self.degree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = torch.tensor(X, dtype= torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype = torch.float32)\n",
    "        K = self.compute_kernel(self.X_train, self.X_train)\n",
    "        super().fit(K, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype = torch.float32)\n",
    "        K = self.compute_kernel(X, self.X_train)\n",
    "        return super().predict(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelLogisticRegression:\n",
    "    '''\n",
    "    The KernelLogisticRegression class extends your original CustomeLogisticRegression \n",
    "    by introducing kernel functions for transforming the input data into a higher-dimensional space.\n",
    "    '''\n",
    "    def __init__(self, learning_rate, C= 1, kernel = 'linear', degree = 3, gamma = None, coef0 = 0 ,**kwargs):\n",
    "        '''\n",
    "        The class takes additional arguments like kernel, degree, gamma, and coef0, \n",
    "        which determine the type of kernel function to be used and its parameters.\n",
    "        '''\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.C = C\n",
    "    \n",
    "    def compute_kernel(self, X1, X2):\n",
    "        if self.kernel == 'linear':\n",
    "            return X1 @ X2.T\n",
    "        elif self.kernel == 'rbf':\n",
    "            gamma = self.gamma or 1/ X1.shape[1]\n",
    "            pairwise_sq_dists = torch.sum(X1**2, dim = 1, keepdim = True) - 2*X1 @X2.T + torch.sum(X2**2, dim = 1)\n",
    "            return torch.exp(-gamma*pairwise_sq_dists)\n",
    "        \n",
    "        elif self.kernel == 'polynomial':\n",
    "            return (X1@X2.T *self.gamma + self.coef0) ** self.degree\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = torch.tensor(X, dtype = torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype = torch.float32)\n",
    "        K = self.compute_kernel(self.X_train, self.X_train)\n",
    "        scaler = StandardScaler().fit(K)\n",
    "        #scaler = MinMaxScaler().fit(K)\n",
    "        K_scaled = scaler.transform(K)\n",
    "        self.scaler = scaler\n",
    "\n",
    "        self.model = LogisticRegression(C = self.C, solver = 'lbfgs', max_iter= 1000)\n",
    "        self.model.fit(K_scaled, y)\n",
    "        #self.model.fit(K, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype= torch.float32)\n",
    "        K = self.compute_kernel(X, self.X_train).numpy()\n",
    "        K_scaled = self.scaler.transform(K)\n",
    "        return self.model.predict(K_scaled)\n",
    "        #return self.model.predict(K)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_linear(x_train, y_train, x_val, y_val, x_test, y_test, reg = 'L2'):\n",
    "    best_lr = None\n",
    "    best_epochs = None\n",
    "    best_lambda = None\n",
    "    best_batch = None\n",
    "    best_model = None\n",
    "    lrs = np.linspace(0.001, 0.041, 2)\n",
    "    epochs_list = np.linspace(50, 250, 2)\n",
    "    lambdas = np.linspace(0.01, 0.21, 2)\n",
    "    batch_sizes = [32, 64]\n",
    "    best_score = -1\n",
    "    Cs = [1,2,3,5, 10, 20]\n",
    "    for lr in lrs:\n",
    "        for epo in epochs_list:\n",
    "            for ld in lambdas:\n",
    "                for batch in batch_sizes:\n",
    "                    for c in Cs:\n",
    "                        if reg == 'L2':\n",
    "                            linear_svm_model = Linear_SVM_Regularize(C = c, learning_rate= lr,epochs= int(epo),batch_size= batch, lmda =  ld)\n",
    "                        elif reg == 'L1':\n",
    "                            linear_svm_model = Linear_SVM_L1(C = c, learning_rate= lr,epochs= int(epo),batch_size= batch, lmda =  ld)\n",
    "                        linear_svm_model.fit(x_train, y_train)\n",
    "                        score = linear_svm_model.score(x_val, y_val).item()\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_lr = lr\n",
    "                            best_epochs = epo\n",
    "                            best_lambda = ld\n",
    "                            best_batch = batch\n",
    "                            best_model = linear_svm_model\n",
    "                        if best_score == 1:\n",
    "                            break\n",
    "    #score on test \n",
    "    print(f'highest test score is {best_model.score(x_test, y_test).item()} with {best_lr} lr, {best_lambda} lambda, {best_batch} best size, and {best_epochs} epochs')\n",
    "    return best_model.score(x_test, y_test).item(), best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_svm_kernel(x_train, y_train, x_val, y_val, x_test, y_test, kernel, model):\n",
    "    best_score = -1\n",
    "    best_model = None\n",
    "\n",
    "    lr = 0.0001\n",
    "    epo = 70\n",
    "    lambdas = np.linspace(0.01, 0.21, 4)\n",
    "    batch  = 64\n",
    "    gammas =  [0.01, 0.1, 1, 10]\n",
    "    coefs = [1, 2, 3, 5, 7, 9]\n",
    "    Cs = [1,2,3,5, 10, 20, 50, 70, 100]\n",
    "    for ld in lambdas:\n",
    "        if kernel == 'polynomial':\n",
    "            for gamma in gammas:\n",
    "                for coef in coefs:\n",
    "                    for degree in [2,3,4]:\n",
    "                        if model == 'KernelSVM':\n",
    "                            kernelsvm_model = KernelSVM(learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, gamma= gamma, degree= degree, coef0= coef, kernel= kernel)\n",
    "                        else:\n",
    "                            kernelsvm_model = KernelLogisticRegression(learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, gamma= gamma, degree= degree, coef0= coef, kernel= kernel)\n",
    "                        kernelsvm_model.fit(x_train, y_train)\n",
    "                        score_kern = kernelsvm_model.score(x_val, y_val).item()\n",
    "                        if score_kern > best_score:\n",
    "                            best_score = score_kern\n",
    "                            best_model = kernelsvm_model\n",
    "        elif kernel =='rbf':\n",
    "            for gamma in [0.0001, 0.001,0.005, 0.01, 0.15, 0.1, 1, 10]:\n",
    "                for c in Cs:\n",
    "                    if model == 'KernelSVM':\n",
    "                        kernelsvm_model = KernelSVM(C = c, learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, gamma= gamma, kernel= kernel)\n",
    "                    else:\n",
    "                        kernelsvm_model = KernelLogisticRegression(C = c, learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, gamma= gamma, kernel= kernel)\n",
    "                    kernelsvm_model.fit(x_train, y_train)\n",
    "                    score_kern = kernelsvm_model.score(x_val, y_val).item()\n",
    "                    if score_kern > best_score:\n",
    "                        best_score = score_kern\n",
    "                        best_model = kernelsvm_model\n",
    "        elif kernel == 'linear':\n",
    "            if model == 'KernelSVM':\n",
    "                kernelsvm_model = KernelSVM(learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, kernel= kernel)\n",
    "            else:\n",
    "                kernelsvm_model = KernelLogisticRegression(learning_rate = lr, epochs = epo, batch_size = batch, lmda = ld, kernel= kernel)\n",
    "            kernelsvm_model.fit(x_train, y_train)\n",
    "            score_kern = kernelsvm_model.score(x_val, y_val).item()\n",
    "            if score_kern > best_score:\n",
    "                best_score = score_kern\n",
    "                best_model = kernelsvm_model\n",
    "            \n",
    "        print(f\"Best score {best_model.score(x_test, y_test)}\")\n",
    "    return best_model.score(x_test, y_test).item(), best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logi_reg(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    best_epoch_lr = None\n",
    "    best_lr_lr = None\n",
    "    best_model = None\n",
    "    best_accuracy = -1\n",
    "    epochs = [100, 300, 600, 800]\n",
    "    learning_rates = np.linspace(0.001, 0.1, 10)\n",
    "    for epoch in epochs:\n",
    "        for lr in learning_rates:\n",
    "            lr_model = CustomeLogisticRegression(learning_rate = lr)\n",
    "            lr_model.fit(x_train, y_train, epoch = epoch)\n",
    "            accuracy = lr_model.accuracy(y_val, x_val)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch_lr = epoch\n",
    "                best_lr_lr = lr\n",
    "                best_model = lr_model\n",
    "    print(f'{best_model.accuracy(y_test, x_test)} with learning rate {best_lr_lr} and {best_epoch_lr} epochs')\n",
    "    return best_accuracy, best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "if np.any(np.isnan(X)):\n",
    "    print(\"There are missing values in the dataset.\")\n",
    "\n",
    "if np.any(np.isinf(X)):\n",
    "    print(\"There are infinity values in the dataset.\")\n",
    "    \n",
    "X = scaler.fit_transform(X)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size= 0.25, random_state= 42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear svm and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest test score is 0.8584070801734924 with 0.001 lr, 0.01 lambda, 64 best size, and 50.0 epochs\n",
      "L2 Regularized SVM Test Accuracy: 0.8584\n",
      "[0.8681871  1.5263065  2.3645935  0.96321374 0.91663784 2.0173714\n",
      " 0.5558926  0.88757825 0.3621307  0.48202047 0.91064143 0.824216\n",
      " 0.21472584 1.2552162  1.5512909  0.53053534 0.7989287  1.8691585\n",
      " 0.03021394 1.4671551  0.62459046 0.45845327 1.4710674  1.006325\n",
      " 0.02441451 0.3871482  0.01346058 1.4597906  1.5284353  0.01301512]\n"
     ]
    }
   ],
   "source": [
    "acc_linear, best_linear_svm = best_linear(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "feature_importance = np.abs(best_linear_svm.W.detach().numpy().flatten())\n",
    "top_important_features = np.argsort(feature_importance)[-10:]\n",
    "print(f\"L2 Regularized SVM Test Accuracy: {acc_linear:.4f}\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest test score is 0.8771929740905762 with 0.001 lr, 0.21 lambda, 64 best size, and 250.0 epochs\n",
      "L1 Regularized SVM Test Accuracy: 0.8772\n",
      "[1.0435293  1.2397331  0.65221024 0.06650675 1.613175   0.45472473\n",
      " 0.72214925 0.02559206 0.09438893 1.257321   0.12434014 0.4478095\n",
      " 0.4296006  1.6613344  0.39254898 0.6565991  1.0370171  0.52156913\n",
      " 0.16434248 0.08603621 1.0617669  0.01676443 0.42827913 0.49123383\n",
      " 1.3109468  0.19145648 1.1907551  0.12759489 0.1954511  0.6430857 ]\n"
     ]
    }
   ],
   "source": [
    "acc_linear_L1, best_linear_L1 = best_linear(X_train, y_train, X_val, y_val, X_test, y_test, reg = \"L1\")\n",
    "feature_importance2 = np.abs(best_linear_L1.W.detach().numpy().flatten())\n",
    "top_important_features2 = np.argsort(feature_importance2)[-10:]\n",
    "print(f\"L1 Regularized SVM Test Accuracy: {acc_linear_L1:.4f}\")\n",
    "print(feature_importance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5964912280701754 with learning rate 0.001 and 100 epochs\n",
      "L1 Regularized SVM Test Accuracy: 0.5965\n"
     ]
    }
   ],
   "source": [
    "acc_logis, best_logis = logi_reg(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "print(f\"L1 Regularized SVM Test Accuracy: {acc_logis:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-81d98a1cf030>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype= torch.float32)\n",
      "<ipython-input-31-81d98a1cf030>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.8157894611358643\n",
      "Best score 0.8157894611358643\n",
      "Best score 0.859649121761322\n",
      "Best score 0.859649121761322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.859649121761322"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Kernel SVM with the selected features (RBF)\n",
    "accuracy_rbf_svm, rbf_svm_model = best_svm_kernel(X_train, y_train, X_val, y_val, X_test, y_test, kernel = 'rbf', model = 'KernelSVM')\n",
    "accuracy_rbf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-81d98a1cf030>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype= torch.float32)\n",
      "<ipython-input-31-81d98a1cf030>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.8508771657943726\n",
      "Best score 0.8508771657943726\n",
      "Best score 0.8508771657943726\n",
      "Best score 0.8859649300575256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8859649300575256"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Kernel SVM with the selected features (Polynomial)\n",
    "accuracy_poly_svm, rbf_poly_model = best_svm_kernel(X_train, y_train, X_val, y_val, X_test, y_test, kernel = 'polynomial', model = 'KernelSVM')\n",
    "accuracy_poly_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 27, 19, 22,  1, 28, 14, 17,  5,  2], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 16,  0, 20, 26,  1,  9, 24,  4, 13], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_important_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top features\n",
    "X_train_selected = X_train[:, top_important_features2]\n",
    "X_val_selected = X_val[:, top_important_features2]\n",
    "X_test_selected = X_test[:, top_important_features2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest test score is 0.7168141603469849 with 0.001 lr, 0.01 lambda, 64 best size, and 50.0 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7168141603469849"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a SVM with the selected features\n",
    "accuracy_rbf_svm, rbf_svm_model = best_linear(X_train_selected, y_train, X_val_selected, y_val, X_test_selected, y_test, reg= 'L1')\n",
    "accuracy_rbf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-81d98a1cf030>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype= torch.float32)\n",
      "<ipython-input-44-81d98a1cf030>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.8070175647735596\n",
      "Best score 0.8070175647735596\n",
      "Best score 0.8684210777282715\n",
      "Best score 0.8684210777282715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8684210777282715"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Kernel SVM with the selected features (RBF)\n",
    "accuracy_rbf_svm, rbf_svm_model = best_svm_kernel(X_train_selected, y_train, X_val_selected, y_val, X_test_selected, y_test, kernel = 'rbf', model = 'KernelSVM')\n",
    "accuracy_rbf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-81d98a1cf030>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype= torch.float32)\n",
      "<ipython-input-44-81d98a1cf030>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.6666666865348816\n",
      "Best score 0.6666666865348816\n",
      "Best score 0.8245614171028137\n",
      "Best score 0.8245614171028137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8245614171028137"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Kernel SVM with the selected features (Polynomial)\n",
    "accuracy_poly_svm, rbf_poly_model = best_svm_kernel(X_train_selected, y_train, X_val_selected, y_val, X_test_selected, y_test, kernel = 'polynomial', model = 'KernelSVM')\n",
    "accuracy_poly_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression with kernel svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.9824561403508771\n",
      "Best score 0.9824561403508771\n",
      "Best score 0.9824561403508771\n",
      "Best score 0.9824561403508771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rbf_logis_svm, rbf_logis_svm_model = best_svm_kernel(X_train, y_train, X_val, y_val, X_test, y_test, 'rbf', model = 'KernelLogisticRegression')\n",
    "accuracy_rbf_logis_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.956140350877193\n",
      "Best score 0.956140350877193\n",
      "Best score 0.956140350877193\n",
      "Best score 0.956140350877193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_poly_logis_svm, poly_logis_svm_model = best_svm_kernel(X_train, y_train, X_val, y_val, X_test, y_test, 'polynomial', model = 'KernelLogisticRegression')\n",
    "accuracy_poly_logis_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn vs neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn Classifiers\n",
    "def knn(X_combined, y_combined, x_test, y_test, split):\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_grid = {'n_neighbors': np.arange(1, 30)}\n",
    "    knn_grid_search = GridSearchCV(knn, knn_grid, cv = split)\n",
    "    knn_grid_search.fit(X_combined, y_combined)\n",
    "\n",
    "    best_k = knn_grid_search.best_estimator_\n",
    "    knn_accuracy = best_k.score(x_test, y_test)\n",
    "    print(knn_accuracy)\n",
    "    return knn_accuracy, best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\thong\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn\n",
    "X_combined = np.vstack((X_train, X_val))\n",
    "y_combined = np.hstack((y_train, y_val))\n",
    "# Create predefinded \n",
    "test_fold = np.concatenate([\n",
    "    np.full(X_train.shape[0], -1),\n",
    "    np.zeros(X_val.shape[0])\n",
    "])\n",
    "predefined_split = PredefinedSplit(test_fold)\n",
    "\n",
    "knn_accuracy, best_knn_model= knn(X_combined, y_combined, X_test, y_test, predefined_split)\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype = torch.float32)\n",
    "        self.y = torch.tensor(y, dtype= torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "        # Initialize\n",
    "        nn.init.uniform_(self.fc1.weight,-1, 1)\n",
    "        nn.init.uniform_(self.fc2.weight,-1, 1)\n",
    "        nn.init.uniform_(self.fc3.weight,-1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Accuracy: 0.876\n",
      "Epoch 2/200, Accuracy: 0.850\n",
      "Epoch 3/200, Accuracy: 0.841\n",
      "Epoch 4/200, Accuracy: 0.876\n",
      "Epoch 5/200, Accuracy: 0.920\n",
      "Epoch 6/200, Accuracy: 0.929\n",
      "Epoch 7/200, Accuracy: 0.956\n",
      "Epoch 8/200, Accuracy: 0.965\n",
      "Epoch 9/200, Accuracy: 0.965\n",
      "Epoch 10/200, Accuracy: 0.965\n",
      "Epoch 11/200, Accuracy: 0.965\n",
      "Epoch 12/200, Accuracy: 0.965\n",
      "Epoch 13/200, Accuracy: 0.965\n",
      "Epoch 14/200, Accuracy: 0.947\n",
      "Epoch 15/200, Accuracy: 0.938\n",
      "Epoch 16/200, Accuracy: 0.912\n",
      "Epoch 17/200, Accuracy: 0.956\n",
      "Epoch 18/200, Accuracy: 0.956\n",
      "Early stopping at epoch 18\n",
      "Fold completed with the test accuracy 0.9557522123893806.\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "batch_size = 128\n",
    "num_epoch = 200\n",
    "learning_rate = 0.015\n",
    "weight_decay = 0.0005\n",
    "dropout_rate = 0.3\n",
    "hidden_size1 = 600\n",
    "hidden_size2= 400\n",
    "early_stopping_patience = 10\n",
    "input_size = X_train_val.shape[1]\n",
    "num_classes = 2\n",
    "\n",
    "skf = StratifiedKFold(n_splits= 5)\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    train_data = CircleData(X_train, y_train)\n",
    "    test_data = CircleData(X_test, y_test)\n",
    "\n",
    "    #Create data loaders\n",
    "    train_loader = DataLoader(dataset= train_data, batch_size= batch_size, shuffle= True)\n",
    "    test_loader = DataLoader(dataset= test_data, batch_size = batch_size, shuffle= False)\n",
    "    \n",
    "    model = Net(input_size= input_size, hidden_size1= hidden_size1, hidden_size2= hidden_size2, num_classes= num_classes, dropout_rate= dropout_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    #train model \n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        #flatten\n",
    "        features = features.reshape(-1, input_size)\n",
    "        #forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features = features.reshape(-1, input_size)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / len(test_data)\n",
    "        test_loss /= len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epoch}, Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    #Early stopping \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "print(f'Fold completed with the test accuracy {accuracy}.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
