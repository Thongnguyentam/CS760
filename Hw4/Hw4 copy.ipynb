{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 0.3333333333333333, 's': 0.3333333333333333, 'j': 0.3333333333333333}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'languageID'\n",
    "alpha = 1/2 \n",
    "K_l = 3\n",
    "num_sample = {'e': 10, 's': 10, 'j' : 10}\n",
    "prior = {}\n",
    "for label in num_sample:\n",
    "    cnt = num_sample[label]\n",
    "    prior[label] = (cnt + alpha)/ (sum(num_sample.values()) + (K_l*alpha))\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = []\n",
    "dict_list = []\n",
    "for label in ['e','s','j']:\n",
    "    alpha_cnt = {}\n",
    "    for i in range (0, 10):\n",
    "        name = label + (str(i)) + \".txt\"\n",
    "        file_path = os.path.join('languageID', name)\n",
    "        with open(file_path,'r') as f:\n",
    "            content = f.read()\n",
    "            for char in content:\n",
    "                if char.isalpha() or char.isspace():\n",
    "                    if char in alpha_cnt:\n",
    "                        alpha_cnt[char] += 1\n",
    "                    else:\n",
    "                        alpha_cnt[char] = 1\n",
    "    #sort keys\n",
    "    if '\\n' in alpha_cnt:\n",
    "        del alpha_cnt['\\n']\n",
    "\n",
    "    sorted_dict = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "    #finding theta\n",
    "    deno = sum(sorted_dict.values()) + alpha*len(sorted_dict)\n",
    "    theta = []\n",
    "    for val in sorted_dict:\n",
    "        i = (sorted_dict[val] + alpha)/ deno\n",
    "        theta.append(i)\n",
    "    dict_list.append(sorted_dict)\n",
    "    lang.append(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1766,\n",
       " 'a': 1885,\n",
       " 'b': 155,\n",
       " 'c': 78,\n",
       " 'd': 246,\n",
       " 'e': 861,\n",
       " 'f': 55,\n",
       " 'g': 200,\n",
       " 'h': 454,\n",
       " 'i': 1388,\n",
       " 'j': 33,\n",
       " 'k': 821,\n",
       " 'l': 20,\n",
       " 'm': 569,\n",
       " 'n': 811,\n",
       " 'o': 1304,\n",
       " 'p': 12,\n",
       " 'q': 1,\n",
       " 'r': 612,\n",
       " 's': 603,\n",
       " 't': 815,\n",
       " 'u': 1010,\n",
       " 'v': 3,\n",
       " 'w': 282,\n",
       " 'y': 202,\n",
       " 'z': 110}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1792499586981662,\n",
       " 0.0601685114819098,\n",
       " 0.011134974392863043,\n",
       " 0.021509995043779945,\n",
       " 0.021972575582355856,\n",
       " 0.1053692383941847,\n",
       " 0.018932760614571286,\n",
       " 0.017478936064761277,\n",
       " 0.047216256401784236,\n",
       " 0.055410540227986124,\n",
       " 0.001420783082768875,\n",
       " 0.0037336857756484387,\n",
       " 0.028977366595076822,\n",
       " 0.020518751032545846,\n",
       " 0.057921691723112505,\n",
       " 0.06446390219725756,\n",
       " 0.01675202378985627,\n",
       " 0.0005617049396993227,\n",
       " 0.053824549810011564,\n",
       " 0.06618205848339666,\n",
       " 0.08012555757475633,\n",
       " 0.026664463902197257,\n",
       " 0.009284652238559392,\n",
       " 0.015496448042293078,\n",
       " 0.001156451346439782,\n",
       " 0.013844374690236246,\n",
       " 0.0006277878737815959]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16826493170115014,\n",
       " 0.10456045141993771,\n",
       " 0.008232863618143134,\n",
       " 0.03752582405722919,\n",
       " 0.039745922111559924,\n",
       " 0.1138108599796491,\n",
       " 0.00860287996053159,\n",
       " 0.0071844839813758445,\n",
       " 0.0045327001942585795,\n",
       " 0.049859702136844375,\n",
       " 0.006629459467793161,\n",
       " 0.0002775122567913416,\n",
       " 0.052943171656748174,\n",
       " 0.02580863988159477,\n",
       " 0.054176559464709693,\n",
       " 0.07249236841293824,\n",
       " 0.02426690512164287,\n",
       " 0.007677839104560451,\n",
       " 0.05929511886774999,\n",
       " 0.06577040485954797,\n",
       " 0.03561407295488884,\n",
       " 0.03370232185254849,\n",
       " 0.00588942678301625,\n",
       " 9.250408559711388e-05,\n",
       " 0.0024976103111220747,\n",
       " 0.007862847275754679,\n",
       " 0.0026826184823163022]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s\n",
    "lang[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12345377035432245,\n",
       " 0.13177021455028304,\n",
       " 0.010867286323293032,\n",
       " 0.005486057725906772,\n",
       " 0.01722692012020407,\n",
       " 0.06020686281361381,\n",
       " 0.003878677755258928,\n",
       " 0.014012160178908379,\n",
       " 0.031763225941715004,\n",
       " 0.09703682996715354,\n",
       " 0.002341183870291425,\n",
       " 0.05741141938640017,\n",
       " 0.0014326647564469914,\n",
       " 0.039800125794954226,\n",
       " 0.05671255852959676,\n",
       " 0.09116639877000489,\n",
       " 0.0008735760710042631,\n",
       " 0.00010482912852051157,\n",
       " 0.04280522747920889,\n",
       " 0.042176252708085823,\n",
       " 0.05699210287231812,\n",
       " 0.07061988957998462,\n",
       " 0.00024460129988119366,\n",
       " 0.019742819204696345,\n",
       " 0.014151932350269061,\n",
       " 0.007722412467677685]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#j\n",
    "lang[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4\n",
    "alpha_cnt = {}\n",
    "name = 'e10.txt'\n",
    "file_path = os.path.join('languageID', name)\n",
    "with open(file_path,'r') as f:\n",
    "    content = f.read()\n",
    "    for char in content:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            if char in alpha_cnt:\n",
    "                alpha_cnt[char] += 1\n",
    "            else:\n",
    "                alpha_cnt[char] = 1\n",
    "#sort keys\n",
    "if '\\n' in alpha_cnt:\n",
    "    del alpha_cnt['\\n']\n",
    "\n",
    "x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "#finding theta\n",
    "deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "theta_test = []\n",
    "for val in x_test:\n",
    "    i = (x_test[val] + alpha)/ deno\n",
    "    theta_test.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 498,\n",
       " 'a': 164,\n",
       " 'b': 32,\n",
       " 'c': 53,\n",
       " 'd': 57,\n",
       " 'e': 311,\n",
       " 'f': 55,\n",
       " 'g': 51,\n",
       " 'h': 140,\n",
       " 'i': 140,\n",
       " 'j': 3,\n",
       " 'k': 6,\n",
       " 'l': 85,\n",
       " 'm': 64,\n",
       " 'n': 139,\n",
       " 'o': 182,\n",
       " 'p': 53,\n",
       " 'q': 3,\n",
       " 'r': 141,\n",
       " 's': 186,\n",
       " 't': 225,\n",
       " 'u': 65,\n",
       " 'v': 31,\n",
       " 'w': 47,\n",
       " 'x': 4,\n",
       " 'y': 38,\n",
       " 'z': 2}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7841.865447060634, -8467.282044010557, -8730.288949147405]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log likelihood\n",
    "p_list = []\n",
    "language = 0\n",
    "for label in ['e', 's', 'j']:\n",
    "    i = 0\n",
    "    p = 0\n",
    "    for x in x_test:\n",
    "        if x == list(dict_list[language].keys())[i]:\n",
    "            p = p + x_test[x]*np.log(lang[language][i])\n",
    "            i = i + 1\n",
    "    p_list.append(p) \n",
    "    language = language + 1\n",
    "p_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['e', 's', 'j'] \n",
    "#log likelihood\n",
    "p_list = []\n",
    "language = 0\n",
    "for label in ['e', 's', 'j']:\n",
    "    i = 0\n",
    "    p = 0\n",
    "    for x in x_test:\n",
    "        if x == list(dict_list[language].keys())[i]:\n",
    "            p = p + x_test[x]*np.log(lang[language][i])\n",
    "            i = i + 1\n",
    "    p_list.append(p) \n",
    "    language = language + 1\n",
    "p_list\n",
    "\n",
    "#log posterior\n",
    "p_list_post = p_list + np.log(list(prior.values()))\n",
    " \n",
    "#show the class label (Choose the class with the highest posterior probability as the predicted class label for x)\n",
    "ind_max = np.argmax(p_list_post)\n",
    "label_list[ind_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#test files 10.txt to 19.txt in three languages\n",
    "ee, ss, jj, ej, es, js, je, se, sj = 0,0,0,0,0,0,0,0,0\n",
    "for label in ['e', 's', 'j']:\n",
    "    for i in range(10, 20):\n",
    "        alpha_cnt = {}\n",
    "        name = label + str(i) + '.txt'\n",
    "        file_path = os.path.join('languageID', name)\n",
    "        with open(file_path,'r') as f:\n",
    "            content = f.read()\n",
    "            for char in content:\n",
    "                if char.isalpha() or char.isspace():\n",
    "                    if char in alpha_cnt:\n",
    "                        alpha_cnt[char] += 1\n",
    "                    else:\n",
    "                        alpha_cnt[char] = 1\n",
    "        #sort keys\n",
    "        if '\\n' in alpha_cnt:\n",
    "            del alpha_cnt['\\n']\n",
    "\n",
    "        x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "        #finding theta\n",
    "        deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "        theta_test = []\n",
    "        for val in x_test:\n",
    "            i = (x_test[val] + alpha)/ deno\n",
    "            theta_test.append(i)\n",
    "        \n",
    "        #log likelihood\n",
    "        p_list = []\n",
    "        language = 0\n",
    "        for label2 in ['e', 's', 'j']:\n",
    "            i = 0\n",
    "            p = 0\n",
    "            for x in x_test:\n",
    "                if x == list(dict_list[language].keys())[i]:\n",
    "                    p = p + x_test[x]*np.log(lang[language][i])\n",
    "                    i = i + 1\n",
    "            p_list.append(p) \n",
    "            language = language + 1\n",
    "        \n",
    "        #log posterior\n",
    "        p_list_post = p_list + np.log(list(prior.values()))\n",
    "\n",
    "        #show the class label (Choose the class with the highest posterior probability as the predicted class label for x)\n",
    "        ind_max = np.argmax(p_list_post)\n",
    "        clas = label_list[ind_max]\n",
    "        #evaluate\n",
    "        if label == 'e' and clas == 'e':\n",
    "            ee += 1\n",
    "        if label == 'e' and clas == 'j':\n",
    "            ej += 1\n",
    "        if label == 'e' and clas == 's':\n",
    "            es += 1\n",
    "        if label == 'j' and clas == 'j':\n",
    "            jj += 1\n",
    "        if label == 'j' and clas == 's':\n",
    "            js += 1\n",
    "        if label == 'j' and clas == 'e':\n",
    "            je += 1\n",
    "        if label == 's' and clas == 's':\n",
    "            ss += 1\n",
    "        if label == 's' and clas == 'j':\n",
    "            sj += 1\n",
    "        if label == 's' and clas == 'e':\n",
    "            se += 1\n",
    "print(ee, ss, jj, ej, es, js, je, se, sj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 498,\n",
       " 'a': 164,\n",
       " 'b': 32,\n",
       " 'c': 53,\n",
       " 'd': 57,\n",
       " 'e': 311,\n",
       " 'f': 55,\n",
       " 'g': 51,\n",
       " 'h': 140,\n",
       " 'i': 140,\n",
       " 'j': 3,\n",
       " 'k': 6,\n",
       " 'l': 85,\n",
       " 'm': 64,\n",
       " 'n': 139,\n",
       " 'o': 182,\n",
       " 'p': 53,\n",
       " 'q': 3,\n",
       " 'r': 141,\n",
       " 's': 186,\n",
       " 't': 225,\n",
       " 'u': 65,\n",
       " 'v': 31,\n",
       " 'w': 47,\n",
       " 'x': 4,\n",
       " 'y': 38,\n",
       " 'z': 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.8\n",
    "alpha_cnt = {}\n",
    "name = 'e10.txt'\n",
    "file_path = os.path.join('languageID', name)\n",
    "with open(file_path,'r') as f:\n",
    "    content = f.read()\n",
    "    shuffled_content = ''.join(random.sample(content, len(content)))\n",
    "    for char in content:\n",
    "        if char.isalpha() or char.isspace():\n",
    "            if char in alpha_cnt:\n",
    "                alpha_cnt[char] += 1\n",
    "            else:\n",
    "                alpha_cnt[char] = 1\n",
    "#sort keys\n",
    "if '\\n' in alpha_cnt:\n",
    "    del alpha_cnt['\\n']\n",
    "\n",
    "x_test = {key: alpha_cnt[key] for key in sorted(alpha_cnt.keys())}\n",
    "#finding theta\n",
    "deno = sum(x_test.values()) + alpha*len(x_test)\n",
    "theta_test = []\n",
    "for val in x_test:\n",
    "    i = (x_test[val] + alpha)/ deno\n",
    "    theta_test.append(i)\n",
    "x_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "#criterion is a PyTorch loss function that computes the loss between the predicted output \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparemeters\n",
    "batch_size = 32\n",
    "batch_size_test = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "input_size = 784  #28x28 pixels\n",
    "hidden_size1 = 300\n",
    "hidden_size2 = 200\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST dataset \n",
    "#TorchVision also offers a lot of handy transformations, such as cropping or normalization.\n",
    "train_dataset = datasets.MNIST(root= './data', train= True, transform= transforms.ToTensor(), download= True)\n",
    "test_dataset = datasets.MNIST(root = './data', train = False, transform= transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data loaders\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size= batch_size, shuffle= True)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "#The torch.randn function generates a tensor of size (hidden_size1, input_size) \n",
    "#filled with random values drawn from a normal distribution with mean 0 and standard deviation 1\n",
    "#The division by np.sqrt(input_size) scales the initial weights by the square root of the input size, \n",
    "#which is a common practice to ensure that the initial weights are not too large or too small.\n",
    "W1 = torch.randn(hidden_size1, input_size) / np.sqrt(input_size)\n",
    "W2 = torch.randn(hidden_size2, hidden_size1) / np.sqrt(hidden_size1)\n",
    "W3 = torch.randn(num_classes, hidden_size2) / np.sqrt(hidden_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    z1 = torch.matmul(W1, x)\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.matmul(W2, a1)\n",
    "    a2 = torch.sigmoid(z2)\n",
    "    z3 = torch.matmul(W3, a2)\n",
    "    y_hat = torch.softmax(z3, dim = 0)\n",
    "    return a1, a2, y_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((num_classes, batch_size))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thong\\AppData\\Local\\Temp\\ipykernel_19000\\3774397082.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(images), torch.tensor(y) #convert 'images' and 'y' into PyTorch tensors.\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # batch_index, (example_data, example_targets)\n",
    "    # labels is a tensor of shape (batch_size,) that contains the true labels for a batch of images.\n",
    "    # images.shape = torch.Size([128, 1, 28, 28])\n",
    "    losses = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #flatten the images:\n",
    "        images = images.reshape(-1, input_size).T #turn into a vector\n",
    "        labels = labels.numpy() # converting the labels tensor from a PyTorch tensor object to a numpy array.\n",
    "\n",
    "        #convert labels to one-hot vectors\n",
    "        y = np.zeros((num_classes, batch_size))\n",
    "        batch_size_i = labels.shape[0]\n",
    "        y[labels, np.arange(batch_size_i)] = 1 #last batch of batch_size = 128 is 96\n",
    "        if batch_size_i < batch_size:\n",
    "            y = np.delete(y, np.arange(batch_size_i, batch_size), axis = 1) #delete the remaining unsed columns\n",
    "        x, y = torch.tensor(images), torch.tensor(y) #convert 'images' and 'y' into PyTorch tensors.\n",
    "\n",
    "        #forward pass \n",
    "        a1, a2, y_hat = forward(x)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = -torch.sum(y * torch.log(y_hat))\n",
    "        losses += loss.item()\n",
    "        #backward pass\n",
    "        delta3 = (y_hat - y).float() #change from double to float \n",
    "        delta2 = torch.matmul(W3.T, delta3)* a2 * (1- a2)\n",
    "        delta1 = torch.matmul(W2.T, delta2) * a1 * (1- a1)\n",
    "\n",
    "        dW3 = torch.matmul(delta3, a2.T) #(y_hat - y)a2^T\n",
    "        dW2 = torch.matmul(delta2, a1.T) # W3.T(y_hat - y)a2(1- a2)a1.T\n",
    "        dw1 = torch.matmul(delta1, x.T) # W2.T*delta2*a1*(1- a1)x.T\n",
    "\n",
    "        #update weights using SGD  \n",
    "        W1 -= learning_rate* dw1 / batch_size\n",
    "        W2 -= learning_rate* dW2 / batch_size\n",
    "        W3 -= learning_rate * dW3 / batch_size\n",
    "    train_loss.append(losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 0, 3, 7, 4, 2, 6, 7, 5, 8, 7, 9, 6, 0, 1, 7, 7, 7, 5, 8, 1,\n",
       "       6, 1, 1, 1, 2, 8, 7, 8, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "np.arange(batch_size_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "test0 = np.zeros((num_classes, batch_size))\n",
    "test0 #10 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for understanding syntax\n",
    "test0[labels, np.arange(batch_size_i)] = 1 #set test0[label[i], i] = 1\n",
    "test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thong\\AppData\\Local\\Temp\\ipykernel_19000\\781778586.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(images), torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0 \n",
    "test_losses = []\n",
    "\n",
    "# is a context manager that temporarily disables gradient computation during model inference. \n",
    "# reduce memory consumption and computation time since gradients are not needed for inference.\n",
    "# is used to compute test accuracy and loss during the testing phase.\n",
    "# when you are sure that you will not call Tensor.backward() \n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        #Flatten the images\n",
    "        images = images.reshape(-1, input_size).T\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        #convert labels into one-hot vectors\n",
    "        y = np.zeros((num_classes, batch_size))\n",
    "        batch_size_j = labels.shape[0]\n",
    "        y[labels,np.arange(batch_size_j)] = 1\n",
    "        if batch_size_j < batch_size:\n",
    "            y = np.delete(y, np.arange(batch_size_j, batch_size), axis = 1)\n",
    "        \n",
    "        x, y = torch.tensor(images), torch.tensor(y)\n",
    "\n",
    "        #forward pass\n",
    "        _, _, y_hat = forward(x)\n",
    "\n",
    "        # compute loss\n",
    "        #This function returns the index of the maximum value in each column of the one-hot encoded y tensor.\n",
    "        #The dim=0 argument specifies that the function should return the index along the first dimension of the tensor, which corresponds to the batch size.\n",
    "        test_loss = criterion(y_hat, torch.argmax(y, dim = 0))\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        #compute predictions and accuracy \n",
    "        # predictions is a tensor of shape (1, batch_size). \n",
    "        # Each element in this tensor represents the predicted class label for the corresponding input image in the batch\n",
    "        #torch.max() is used to get the index of the maximum value in the tensor y_hat along the 0-th dimension\n",
    "        \n",
    "        _, predictions = torch.max(y_hat, 0)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        total += y.size(1)\n",
    "        correct += (predictions == labels).sum().item()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first dimension = 10 = batch_test_size ->dim = 0 \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim 1 x 10, (batch_size,)\n",
    "torch.argmax(y, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7950e-06, 1.0077e-03, 1.6188e-04, 8.2712e-01, 3.1803e-07, 1.4093e-05,\n",
       "         2.7877e-04, 2.2274e-06, 3.6304e-03, 1.2768e-03],\n",
       "        [1.0209e-04, 2.8491e-05, 1.1066e-06, 1.3787e-07, 9.7752e-01, 8.7183e-05,\n",
       "         4.6971e-05, 4.5300e-05, 4.1221e-03, 2.8811e-07],\n",
       "        [3.6985e-04, 5.2429e-03, 8.3877e-05, 1.3700e-04, 1.1577e-02, 9.9244e-01,\n",
       "         2.8962e-03, 8.1360e-05, 2.2597e-03, 5.4735e-03],\n",
       "        [2.3064e-03, 1.5788e-01, 4.5831e-05, 3.2359e-04, 1.9485e-03, 5.6665e-03,\n",
       "         9.9569e-01, 1.2401e-04, 6.1460e-03, 7.8874e-07],\n",
       "        [1.2725e-05, 5.1942e-06, 5.0506e-01, 3.4314e-06, 2.3034e-05, 1.9848e-07,\n",
       "         1.4108e-08, 7.4706e-01, 7.9489e-03, 3.8632e-04],\n",
       "        [4.2034e-05, 2.1159e-02, 2.0766e-03, 1.6933e-01, 4.0312e-04, 1.2286e-05,\n",
       "         5.6625e-04, 2.7190e-03, 7.5546e-01, 2.5710e-04],\n",
       "        [2.8408e-08, 2.0737e-06, 4.2517e-04, 2.5782e-04, 4.3001e-04, 2.2009e-05,\n",
       "         1.1695e-07, 4.5659e-04, 2.0575e-02, 9.9252e-01],\n",
       "        [9.6345e-01, 5.9457e-05, 5.6157e-03, 1.1751e-06, 2.2564e-04, 5.5439e-06,\n",
       "         3.5710e-04, 9.3528e-04, 3.0361e-04, 2.8109e-08],\n",
       "        [3.7356e-04, 8.1326e-01, 1.9793e-03, 2.8171e-03, 7.7392e-03, 1.7466e-03,\n",
       "         1.5475e-04, 1.8556e-02, 1.9634e-01, 7.7880e-05],\n",
       "        [3.3343e-02, 1.3597e-03, 4.8455e-01, 1.1195e-05, 1.3509e-04, 3.6548e-06,\n",
       "         1.3462e-05, 2.3002e-01, 3.2177e-03, 2.2678e-06]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim 10 x 10, (num_classes, batch_size)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4444947242736816"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y_hat, torch.argmax(y, dim = 0)).item() #item() for getting the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 2.274928773999214\n"
     ]
    }
   ],
   "source": [
    "# Print test accuracy and loss\n",
    "test_accuracy = 100 * correct / total\n",
    "test_loss = np.mean(test_losses)\n",
    "print(test_accuracy, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label = \"Test losses\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network (but with auto-grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model, loss function, and optimizer\n",
    "model = Net(input_size= 784, hidden_size1= 300, hidden_size2 = 200, num_classes = 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Accuracy: 0.114\n",
      "Epoch 2/30, Accuracy: 0.114\n",
      "Epoch 3/30, Accuracy: 0.205\n",
      "Epoch 4/30, Accuracy: 0.315\n",
      "Epoch 5/30, Accuracy: 0.369\n",
      "Epoch 6/30, Accuracy: 0.400\n",
      "Epoch 7/30, Accuracy: 0.503\n",
      "Epoch 8/30, Accuracy: 0.575\n",
      "Epoch 9/30, Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "total_step = len(train_loader)\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "test_loss_list = []\n",
    "test_accuracy_list = []\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train() #sets the model to training mode. \n",
    "    ##new## \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ####\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #flatten the images\n",
    "        images = images.reshape(-1, input_size)\n",
    "\n",
    "        #forward pass \n",
    "        outputs = model(images)\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #track loss and accuracy \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        train_acc += (predicted == labels).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_acc)\n",
    "\n",
    "    #evaluate the model\n",
    "    model.eval() #to switch the model to evaluation mode.\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, input_size)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        test_accuracy_list.append(accuracy)\n",
    "        test_loss_list.append(test_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7671"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num epoch = 25, lr = 0.15, batch_size = 64, accuracy:76.03\n",
    "# num epoch = 25, lr = 0.1, batch_size = 64, accuracy:74.89\n",
    "# num epoch = 20, lr = 0.08, batch_size = 64, accuracy: 66.54\n",
    "# num epoch = 20, lr = 0.15, batch_size = 32, accuracy: \n",
    "# num epoch = 20, lr = 0.1, batch_size = 32, accuracy: \n",
    "# num epoch = 20, lr = 0.07, batch_size = 32, accuracy: 75.57\n",
    "# num epoch = 20, lr = 0.06, batch_size = 32, accuracy:\n",
    "# num epoch = 25, lr = 0.15, batch_size = 32, accuracy: \n",
    "# num epoch = 25, lr = 0.1, batch_size = 32, accuracy: 76.56\n",
    "# num epoch = 25, lr = 0.08, batch_size = 32, accuracy:\n",
    "# num epoch = 25, lr = 0.05, batch_size = 32, accuracy: 75.24\n",
    "# num epoch = 30, lr = 0.1, batch_size = 32, accuracy: \n",
    "# num epoch = 30, lr = 0.08, batch_size = 32, accuracy:\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.035975064818064374,\n",
       " 0.03594226524035136,\n",
       " 0.035724158120155336,\n",
       " 0.03424738771915436,\n",
       " 0.03275102691253026,\n",
       " 0.031538014429807665,\n",
       " 0.030382532401879627,\n",
       " 0.029530042469501496,\n",
       " 0.02899958557685216,\n",
       " 0.02864735690156619,\n",
       " 0.02843328335483869,\n",
       " 0.028298077617088954,\n",
       " 0.028208778566122056,\n",
       " 0.028144635021686553,\n",
       " 0.028098802141348522,\n",
       " 0.02786064531604449,\n",
       " 0.027439339631795883,\n",
       " 0.027263890035947164,\n",
       " 0.02713167005777359,\n",
       " 0.027027917712926865,\n",
       " 0.026945104372501374,\n",
       " 0.026878648815552394,\n",
       " 0.02682575561205546,\n",
       " 0.02678235943118731,\n",
       " 0.026746917291482288,\n",
       " 0.026715581719080606,\n",
       " 0.026690109997987748,\n",
       " 0.026667861898740132,\n",
       " 0.026645962872107822,\n",
       " 0.026629097924629846,\n",
       " 0.02661156112353007,\n",
       " 0.026596804817517597,\n",
       " 0.02658342442512512,\n",
       " 0.026570123094320296,\n",
       " 0.026557992778221767,\n",
       " 0.026547027627627055,\n",
       " 0.02653772277235985,\n",
       " 0.026526369243860246,\n",
       " 0.026518923715750375,\n",
       " 0.02650843338370323]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11078333333333333,\n",
       " 0.11236666666666667,\n",
       " 0.1133,\n",
       " 0.11326666666666667,\n",
       " 0.16865,\n",
       " 0.22623333333333334,\n",
       " 0.3095333333333333,\n",
       " 0.36593333333333333,\n",
       " 0.3923833333333333,\n",
       " 0.41718333333333335,\n",
       " 0.47955,\n",
       " 0.5232833333333333,\n",
       " 0.5637333333333333,\n",
       " 0.5951166666666666,\n",
       " 0.6153333333333333,\n",
       " 0.63115,\n",
       " 0.6417833333333334,\n",
       " 0.6490333333333334,\n",
       " 0.6542666666666667,\n",
       " 0.65795,\n",
       " 0.6606,\n",
       " 0.6632833333333333,\n",
       " 0.7035333333333333,\n",
       " 0.7156833333333333,\n",
       " 0.7211833333333333,\n",
       " 0.7266,\n",
       " 0.7305166666666667,\n",
       " 0.7341166666666666,\n",
       " 0.7377166666666667,\n",
       " 0.7404,\n",
       " 0.7428833333333333,\n",
       " 0.7447333333333334,\n",
       " 0.74675,\n",
       " 0.7486833333333334,\n",
       " 0.7498333333333334,\n",
       " 0.7514,\n",
       " 0.7520833333333333,\n",
       " 0.7532833333333333,\n",
       " 0.75425,\n",
       " 0.7552833333333333,\n",
       " 0.7559666666666667,\n",
       " 0.75695,\n",
       " 0.7574666666666666,\n",
       " 0.7583666666666666,\n",
       " 0.7586,\n",
       " 0.7594166666666666,\n",
       " 0.7598166666666667,\n",
       " 0.7604166666666666,\n",
       " 0.7606833333333334,\n",
       " 0.76115,\n",
       " 0.7617,\n",
       " 0.7623,\n",
       " 0.7622666666666666,\n",
       " 0.7629833333333333,\n",
       " 0.7633166666666666,\n",
       " 0.7636833333333334,\n",
       " 0.76415,\n",
       " 0.7647,\n",
       " 0.76465,\n",
       " 0.7654333333333333,\n",
       " 0.7656666666666667,\n",
       " 0.76595,\n",
       " 0.76605,\n",
       " 0.76665,\n",
       " 0.7669666666666667,\n",
       " 0.7669166666666667,\n",
       " 0.7671666666666667,\n",
       " 0.7675166666666666,\n",
       " 0.7677333333333334,\n",
       " 0.7679333333333334,\n",
       " 0.7681166666666667,\n",
       " 0.7689,\n",
       " 0.76865,\n",
       " 0.7688166666666667,\n",
       " 0.7695833333333333,\n",
       " 0.7695666666666666,\n",
       " 0.76955,\n",
       " 0.7699,\n",
       " 0.7703833333333333,\n",
       " 0.7705666666666666,\n",
       " 0.77055,\n",
       " 0.7706666666666667,\n",
       " 0.7707833333333334,\n",
       " 0.7712333333333333,\n",
       " 0.7712833333333333,\n",
       " 0.7716666666666666,\n",
       " 0.7718333333333334,\n",
       " 0.7716333333333333,\n",
       " 0.7722666666666667,\n",
       " 0.7721833333333333,\n",
       " 0.7725166666666666,\n",
       " 0.7728,\n",
       " 0.77305,\n",
       " 0.77325,\n",
       " 0.7732333333333333,\n",
       " 0.7734333333333333,\n",
       " 0.77345,\n",
       " 0.7736833333333333,\n",
       " 0.7741166666666667,\n",
       " 0.7740666666666667]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracy_list, label = \"training loss\", color = 'orange')\n",
    "plt.plot(test_accuracy_list, label = \"test loss\", color = 'blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "         # Initialize weights to zero\n",
    "        nn.init.zeros_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
