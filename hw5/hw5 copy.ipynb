{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Experiment (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data, k, max_iter = 100, n_restarts = 100):\n",
    "    np.random.seed(0)\n",
    "    objective_min = np.inf\n",
    "    centroid_best = None\n",
    "    labels_best = None\n",
    "    for restart in range(n_restarts):\n",
    "        centroids = data[np.random.choice(data.shape[0], k, replace = False), :]\n",
    "        for i in range(max_iter):\n",
    "            #data.shape = (n_sample, n_features) = (1500, 2)\n",
    "            #centroids.shape = (n_clusters, n_features) = (5, 2)\n",
    "            # (n_clusters, n_samples, n_features) = (5, 1, 2)\n",
    "            # data - centroids[:, np.newaxis] = (n_clusters, n_samples, n_features)\n",
    "            # sum(axis = 2) result resulting in an array of shape (n_clusters, n_samples) containing the squared \n",
    "            #    Euclidean distances between each data point and each centroid.\n",
    "            labels = np.argmin(((data - centroids[:, np.newaxis])**2).sum(axis = 2), axis = 0)\n",
    "            # returns the indices of the minimum values along columns.\n",
    "            for j in range(k):\n",
    "                # choose columns with a row value of true\n",
    "                # in this case, there're 5 centroids\n",
    "                centroids[j] = data[labels == j, :].mean(axis = 0)\n",
    "        # picking the one with the lowest objective value\n",
    "        objective = np.sum(((data - centroids[labels, :])**2).sum(axis = 1))\n",
    "        if objective_min > objective:\n",
    "            objective_min = objective\n",
    "            centroid_best = centroids\n",
    "            labels_best = labels\n",
    "\n",
    "    return centroid_best, labels_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-means++ initialization strategy\n",
    "# k-means++ algorithm selects the initial centroids to be well-spaced and tries to improve the convergence of the k-means algorithm.\n",
    "\n",
    "def kmean_plus(data, k, max_iter = 100, n_restarts = 1000):\n",
    "    \"\"\"\n",
    "    kmeans.labels_ : returns an array of cluster labels, where each element corresponds \n",
    "        to a data point and specifies which cluster it belongs to.\n",
    "    centroids[labels] uses these labels to index the centroids array, so that \n",
    "        each data point is paired with its assigned centroid.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    objective_min = np.inf\n",
    "    centroid_best = None\n",
    "\n",
    "    for restart in range(n_restarts):\n",
    "        kmeans = KMeans(n_clusters= k, init = 'k-means++', max_iter = max_iter).fit(data)\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        # calculates the sum of squared distances between each data point and its assigned centroid.\n",
    "        objective = ((data - centroids[labels])).sum()\n",
    "\n",
    "        if objective_min > objective:\n",
    "            objective_min = objective\n",
    "            centroid_best = centroids\n",
    "            labels_best = labels\n",
    "        \n",
    "    return centroid_best, labels_best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation maximization algorith for GMMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsibility matrix for each sample and cluster\n",
    "def compute_respon(data, k, means, covariances, weights):\n",
    "    n_data, n_features = data.shape\n",
    "    respon = np.zeros((n_data, k))\n",
    "    for j in range(k):\n",
    "        # calculates the probability density function (PDF) of a multivariate normal distribution \n",
    "        # at a given set of points.\n",
    "        # used in the E-step to compute the responsibilities of each data point for each mixture component.\n",
    "        # calculating the posterior probability of each data point being in the j-th cluster. \n",
    "        respon[:, j] = weights[j] * multivariate_normal.pdf(data, mean = means[j], cov = covariances[j])\n",
    "    # respon has shape (n_data, k), normal has shape (n_data, 1)\n",
    "    # Therefore, we need to add extra axis so that it has the same shape as respon\n",
    "    normal = np.sum(respon, axis = 1)[:, np.newaxis]\n",
    "    #normalize the responsibility across each cluster\n",
    "    return respon/normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm(data, k, max_iter = 100, n_restarts = 10):\n",
    "    np.random.seed(0)\n",
    "    obj_min = np.inf\n",
    "    best_means = None\n",
    "    best_covs = None\n",
    "    best_weights = None\n",
    "    best_labels = None\n",
    "    n_data, n_features = data.shape\n",
    "\n",
    "    for ind in range(n_restarts):\n",
    "        # Initialize parameters\n",
    "        # the weights are initialized uniformly.\n",
    "        # the means are randomly chosen from the input data.\n",
    "        # the covariance matrices are initialized as the identity matrix.\n",
    "        weights = np.ones(k) / k\n",
    "        means = data[np.random.choice(n_data, k, replace = False), :]\n",
    "        cov = np.array([np.eye(n_features) for i in range (k)])\n",
    "        # E-step: compute responsibilities\n",
    "        for i in range(max_iter):\n",
    "            likelihoods = np.array([multivariate_normal.pdf(data, means[j], cov[j], allow_singular= True) for j in range(k)]).T \n",
    "            # weights = (k,)\n",
    "            # (n_data, k): The probability of each data belong to one cluster\n",
    "            joint_prob = likelihoods * weights\n",
    "            respon = joint_prob / joint_prob.sum(axis = 1)[:, np.newaxis, :]\n",
    "            #respon = compute_respon(data, k, means, cov, weights)\n",
    "            # Nk[i] represents the sum of responsibilities for cluster 'i' over all data points. \n",
    "            # This value is used in the M-step of the EM algorithm to update the \n",
    "            # cluster weights, means, and covariances.\n",
    "            Nk = np.sum(respon, axis = 0)\n",
    "            # ensures that the weights sum up to 1, \n",
    "            # which is a requirement for probabilities.\n",
    "            # Updates the prior probabilities of the data points\n",
    "            # belonging to each of the Gaussian mixture components.\n",
    "            weights = Nk/n_data\n",
    "            # updated based on the current responsibilities and the data.\n",
    "            # data = (1500, 2) = (n_data, n_features)\n",
    "            # respon = (1500, 5) = (n_data, k)'\n",
    "            # Weighted sum of the data points, where the weights are given by\n",
    "            # the responsibilities of the Gaussian components for each data point.\n",
    "            # Nk[:, np.newaxis] has shape (k, 1)\n",
    "            # divide each row of the weighted sum by the corresponding value of Nk\n",
    "            means = (respon.T @ data) / Nk[: , np.newaxis]\n",
    "            # means = (k,n_features)\n",
    "            for j in range(k):\n",
    "                # (num_data, num_features)\n",
    "                diff = data - means[j]\n",
    "                cov[j] = (respon[:, j, np.newaxis]).T @ diff / Nk[j]\n",
    "        objective = -np.log(likelihoods.sum(axis = 1)).sum()\n",
    "        if objective < obj_min:\n",
    "            obj_min = objective\n",
    "            best_covs = cov\n",
    "            best_means = means\n",
    "            best_weights = weights\n",
    "    labels = np.argmax(respon, axis = 1)\n",
    "    return best_means, best_covs, best_weights, obj_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]]])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.eye(2) for i in range (5)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.5, 1, 2, 4, 8]\n",
    "mu1 = np.array([-1, -1])\n",
    "mu2 = np.array([1, -1])\n",
    "mu3 = np.array([0, 1])\n",
    "sigma1 = np.array([[2, 0.5], [0.5, 1]])\n",
    "sigma2 = np.array([[1, -0.5], [-0.5, 2]])\n",
    "sigma3 = np.array([[1, 0], [0, 2]])\n",
    "np.random.seed(0)\n",
    "numpoints = 100\n",
    "data = np.empty((0, 2), float)\n",
    "true_label = []\n",
    "for sigma in sigmas:\n",
    "    multi = np.random.multivariate_normal(mu1, sigma*sigma1, numpoints)\n",
    "    multi2 = np.random.multivariate_normal(mu2, sigma*sigma2, numpoints)\n",
    "    multi3 = np.random.multivariate_normal(mu3, sigma*sigma3, numpoints)\n",
    "    dataset = np.concatenate((multi, multi2, multi3))\n",
    "    true_label.extend([0] * 100)\n",
    "    true_label.extend([1] * 100)\n",
    "    true_label.extend([2] * 100)\n",
    "    data = np.concatenate((data, dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_accuracy(means, true_labels):\n",
    "    labels_pred = np.zeros_like(true_labels)\n",
    "    for i in range (len(means)):\n",
    "        dist = np.sqrt(((means[i] - [mu1, mu2, mu3])**2).sum(axis = 1))\n",
    "        labels_pred[labels_pred == i] = np.argmin(dist) #find the min mu from the 3 mu\n",
    "    return np.mean(labels_pred == true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_cluster_accuracy(centroids, labels, true_labels):\n",
    "    \"\"\"\n",
    "    labels: an array containing index of  cluster produced by one of the above \n",
    "    true_labels: array containing the true cluster assignments \n",
    "    \"\"\"\n",
    "    true_means = np.array([[-1, -1], [1, -1], [0, 1]])\n",
    "    mapping = []\n",
    "    for i in range(3):\n",
    "        idx = (labels == i)\n",
    "        idx = np.nonzero(idx)[0] # get the index \n",
    "        # if a cluster is empty, it is assigned a mapping of -1 \n",
    "        if np.sum(idx) == 0:\n",
    "            mapping.append(-1)\n",
    "            # to skip the current iteration of the loop and move on to the next iteration.\n",
    "            continue\n",
    "        distances = np.linalg.norm(centroids[i] - true_means, axis = 1)\n",
    "        closest_means = np.argmin(distances) #location of the closest true mean\n",
    "        mapping.append(closest_means)\n",
    "    predicted_labels = np.array([mapping[label] for label in labels])\n",
    "    accuracy = (predicted_labels == true_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_best, labels_best = k_means(data, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.642"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmean_cluster_accuracy(centroid_best, labels_best, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_best_plus, labels_best_plus = kmean_plus(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6433333333333333"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmean_cluster_accuracy(centroid_best_plus, labels_best_plus, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
